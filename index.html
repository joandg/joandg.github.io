<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Joan Duran</title> <meta name="author" content="Joan Duran"/> <meta name="description" content="Joan Duran's personal webpage. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/uib.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://joandg.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">Home<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/Projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/Mentoring/">Mentoring</a> </li> <li class="nav-item "> <a class="nav-link" href="/Teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Joan</span> Duran </h1> <p class="desc">Associate Professor of Applied Mathematics <br> Department of Mathematics and Computer Science <br> <a href="http://iac3.uib.es/" target="_blank" rel="noopener noreferrer">Institute of Applied Computing and Community Code (IAC3)</a> <br> <a href="https://www.uib.eu/" target="_blank" rel="noopener noreferrer">University of the Balearic Islands</a></p> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%6F%61%6E.%64%75%72%61%6E@%75%69%62.%65%73" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-0043-1663" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=IgKAJBwAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Joan_Duran2/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/TAMI-UIB" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/joan-duran-grimalt-82b3742a0" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/joan_dg" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="http://personal.uib.eu/joan.duran" title="Work" target="_blank" rel="noopener noreferrer"><i class="fas fa-briefcase"></i></a> </div> <div class="contact-note"> </div> </div> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/joanduran.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="joanduran.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <span style="text-align: justify"> <p>I am an associate professor of Applied Mathematics in the Department of Mathematics and Computer Science at the <a href="http://www.uib.eu/" target="_blank" rel="noopener noreferrer">University of the Balearic Islands (UIB)</a>. I am a member of the Mathematical Image Processing (TAMI) research group and of the <a href="http://iac3.uib.es" target="_blank" rel="noopener noreferrer">Institute of Applied Computing and Community Code (IAC3)</a>. From July 2021, I am deputy director of the <a href="http://eps.uib.es/" target="_blank" rel="noopener noreferrer">Higher Polytechnic School</a> and head of studies of the Degree in Mathematics.</p> <p>My main research interests currently include nonlinear analysis, calculus of variations, partial differential equations, convex optimization, and deep unfolding architectures, with applications in image processing, satellite imaging, and computer vision.</p> <p>I have participated in several national and international research projects, leading two of them. I have collaborated with the <a href="http://cnes.fr/en" target="_blank" rel="noopener noreferrer">National Centre for Space Studies</a> (CNES) in France by contributing to the image restoration chain of Earth observation satellites, and with the <a href="http://www.ba.ieo.es" target="_blank" rel="noopener noreferrer">Oceanographic Centre of the Balearic Islands</a>, working on deep unfolding architectures for the fusion of remote sensing data and the automatic detection of floating objects in the Mediterranean Sea.</p> <p>I have been a visiting researcher in the groups led by <a href="http://vision.in.tum.de/" target="_blank" rel="noopener noreferrer">Prof. Daniel Cremers</a> at the Technical University of Munich, <a href="http://sites.google.com/site/jeanmichelmorelcmlaenscachan/" target="_blank" rel="noopener noreferrer">Prof. Jean-Michel Morel</a> at Centre de Mathématiques et Leurs Applications at ENS Paris-Saclay, and <a href="http://cims.nyu.edu/~bruna/" target="_blank" rel="noopener noreferrer">Prof. Joan Bruna</a> at the Courant Institute of Mathematical Sciences and the Center for Data Science at New York University.</p> <p>If you are BSc/MSc/PhD student looking for a research internship or a UIB student interested in pursuing your degree/master thesis with me, please visit my <a href="/Mentoring/">mentoring page</a>.</p> </span> </div> <div style="margin-top:5px;"> <div class="news"> <h2>News</h2> <div class="table-responsive" style="max-height: qqvw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Apr 19, 2024</th> <td> The chapter <i><a href="Publications/#PereiraSansSR4RS">A comprehensive overview of satellite image fusion: From classical model-based to cutting-edge deep learning approaches</a></i> has been accepted to “Super Resolution for Remote Sensing” by Springer Nature. </td> </tr> <tr> <th scope="row">Dec 7, 2023</th> <td> <a href="Publications/#HammondSbertVISAPP2024">Three conference papers</a> accepted at <a href="https://visapp.scitevents.org/" target="_blank" rel="noopener noreferrer">VISAPP 2024</a>, to be held in Rome, Italy. </td> </tr> <tr> <th scope="row">Dec 4, 2023</th> <td> <a href="Publications/#PereiraSansMIGARS2024">Two short conference papers</a> accepted at <a href="https://conferences.co.nz/migars2024/" target="_blank" rel="noopener noreferrer">MIGARS 2024</a>, to be held in Wellington, New Zealand. </td> </tr> <tr> <th scope="row">Oct 10, 2023</th> <td> M. Francesc Alcover and Daniel Torres started their PhD on <i>Nonlocal theory for variational problems and partial differential equations</i> and on <i>Combining variational models and deep learning for image processing problems</i>, respectively. </td> </tr> <tr> <th scope="row">Apr 4, 2023</th> <td> <i><a href="Publications/#MifdalTomasCVPR2023">Deep unfolding for hypersharpening using a high-frequency injection module</a></i> accepted at <a href="https://www.grss-ieee.org/events/earthvision-2023/" target="_blank" rel="noopener noreferrer">CVPR Earthvision Workshop 2023</a>, to be held in Vancouver, Canada. </td> </tr> <tr> <th scope="row">Apr 3, 2023</th> <td> <i><a href="Publications/#TomasMifdalIGARSS2023">End-to-end shallow network for variational pansharpening</a></i> accepted at <a href="https://2023.ieeeigarss.org/" target="_blank" rel="noopener noreferrer">IGARSS 2023</a>, to be held in Pasadena, California. </td> </tr> </table> </div> </div> </div> <div style="margin-top:5px;"> <div class="publications" style="margin-top:-5px;"> <h2>Recent Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#E00909"><a href="">SR4RS</a></abbr></div> <div id="PereiraSansSR4RS" class="col-sm-8"> <div class="title">A comprehensive overview of satellite image fusion: From classical model-based to cutting-edge deep learning approaches</div> <div class="author"> Pereira-Sánchez, I., Sans, E., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Super Resolution for Remote Sensing,</em> Springer Nature, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Earth observation satellites usually acquire a high-resolution image with a very limited number of spectral bands along with a lower-resolution image that accurately encodes the spectral responses of objects in the scene. Satellite image fusion, also known as pansharpening or hypersharpening depending on the characteristics of the data, aims to combine the spatial and spectral information into a single high-resolution multispectral or hyperspectral image. The resulting image is then used in a wide variety of remote sensing applications, for which low ground sampling distance and detailed description of the chemical-physical composition of the objects may be required. In this chapter, we review the state of the art of satellite image fusion, emphasizing the crucial role that the modeling of the problem plays in performance and analyzing how deep learning has changed the paradigm. This study enables comprehending the evolution of various approaches and their respective outcomes. We establish a fair comparison process, standardizing a general strategy to train and evaluate fusion methods. Quantitative and qualitative comparisons are conducted on several datasets with distinct resolutions and sensor characteristics. The source code used for the comparison are published freely for non-commercial use.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">PereiraSansSR4RS</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pereira-S\'anchez, I. and Sans, E. and Navarro, J. and Duran, J.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A comprehensive overview of satellite image fusion: From classical model-based to cutting-edge deep learning approaches}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Super Resolution for Remote Sensing}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Kawulok, M. and Kawulok, J. and Smolka, B. and Emre Celebi, M.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">MIGARS</a></abbr></div> <div id="PereiraSansMIGARS2024" class="col-sm-8"> <div class="title">A simple nonlocal back-projection unfolded network for pansharpening</div> <div class="author"> Pereira-Sánchez, I., Sans, E., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Machine Intelligence for Geoanalytics and Remote Sensing (MIGARS),</em> Wellington, New Zealand, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Pansharpening is the fusion process that combines the geometry of a high-resolution panchromatic image with the spectral information encoded in a low-resolution multispectral image. We introduce a back-projection method to minimize the reconstruction error between the target image and the output produced by the Brovey pansharpening model. We replace the back-projection kernel with a residual network that incorporates a nonlocal module, exploiting self-similarity and built upon the multi-head attention mechanism. Experimental validation showcases that our method achieves state-of-the-art results.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">PereiraSansMIGARS2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A simple nonlocal back-projection unfolded network for pansharpening}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pereira-S\'anchez, I. and Sans, E. and Navarro, J. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Machine Intelligence for Geoanalytics and Remote Sensing (MIGARS)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Wellington, New Zealand}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">VISAPP</a></abbr></div> <div id="PereiraSansVISAPP2024" class="col-sm-8"> <div class="title">Beyond variational models and self-similarity in super-resolution: Unfolding models and multi-head attention</div> <div class="author"> Pereira-Sánchez, I., Sans, E., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP),</em> Rome, Italy, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Classical variational methods for solving image processing problems are more interpretable and flexible than pure deep learning approaches, but their performance is limited by the use of rigid priors. Deep unfolding networks combine the strengths of both by unfolding the steps of the optimization algorithm used to estimate the minimizer of an energy functional into a deep learning framework. In this paper, we propose an unfolding approach to extend a variational model exploiting self-similarity of natural images in the data fidelity term for single-image super-resolution. The proximal, downsampling and upsampling operators are written in terms of a neural network specifically designed for each purpose. Moreover, we include a new multi-head attention module to replace the nonlocal term in the original formulation. A comprehensive evaluation covering a wide range of sampling factors and noise realizations proves the benefits of the proposed unfolding techniques. The model shows to better preserve image geometry while being robust to noise.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">PereiraSansVISAPP2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond variational models and self-similarity in super-resolution: Unfolding models and multi-head attention}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pereira-S\'anchez, I. and Sans, E. and Navarro, J. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Rome, Italy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">VISAPP</a></abbr></div> <div id="TorresSbertVISAPP2024" class="col-sm-8"> <div class="title">Combining total variation and nonlocal variational models for low-light image enhancement</div> <div class="author"> Torres, D., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP),</em> Rome, Italy, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The rapid expansion of technology has popularized the use of image processing techniques in several fields. However, images captured under low-light conditions impose significant limitations on the performance of these methods. Therefore, improving the quality of these images by discounting the effect of the illumination is crucial. In this paper, we present a low-light image enhancement method based on the Retinex theory. Our approach estimates illumination and reflectance in two steps. First, the illumination is obtained as the minimizer of an energy functional involving total variation regularization, which favours piecewise smooth solutions. Afterwards, the reflectance component is computed as the minimizer of an energy involving contrast-invariant nonlocal regularization and a fidelity term preserving the largest gradients of the input image.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TorresSbertVISAPP2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Combining total variation and nonlocal variational models for low-light image enhancement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Torres, D. and Sbert, C. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Rome, Italy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">CVPR</a></abbr></div> <div id="MifdalTomasCVPR2023" class="col-sm-8"> <div class="title">Deep unfolding for hypersharpening using a high-frequency injection module</div> <div class="author"> <a href="https://scholar.google.co.uk/citations?hl=en&amp;user=fWqPN8oAAAAJ&amp;view_op=list_works&amp;authuser=1" target="_blank" rel="noopener noreferrer">Mifdal, J.</a>, Tomás-Cruz, M., <a href="https://alessandrosebastianelli.github.io/" target="_blank" rel="noopener noreferrer">Sebastianelli, A.</a>, Coll, B.,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Earthvision Workshop,</em> Vancouver, Canada, 2023 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10208956" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2023-cvpr.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>The fusion of multi-source data with different spatial and spectral resolutions is a crucial task in many remote sensing and computer vision applications. Model-based fusion methods are more interpretable and flexible than pure data-driven learning networks, however, their performance depends greatly on the established fusion model and the hand-crafted prior. In this work, we propose an end-to-end trainable model-based network for hyperspectral and panchromatic image fusion. We introduce an energy functional that takes into account classical observation models and incorporates a high-frequency details injection constraint. The resulting optimization function is solved by a forward-backward splitting algorithm and unfolded into a deep-learning framework that uses two modules trained in parallel to ensure both data observation fitting and constraint compliance. Extensive experiments are conducted on the remote-sensing hyperspectral PRISMA dataset and on the CAVE dataset, proving the superiority of the proposed deep unfolding network both qualitatively and quantitatively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">MifdalTomasCVPR2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep unfolding for hypersharpening using a high-frequency injection module}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mifdal, J. and Tom{\'a}s-Cruz, M. and Sebastianelli, A. and Coll, B. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Earthvision Workshop}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2105--2114}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPRW59228.2023.00204}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Joan Duran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>