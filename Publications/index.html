<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Joan Duran</title> <meta name="author" content="Joan Duran"/> <meta name="description" content="Joan Duran's personal webpage. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/uib.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://joandg.github.io/Publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Joan </span>Duran</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home</a> </li> <li class="nav-item active"> <a class="nav-link" href="/Publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/Mentoring/">Mentoring</a> </li> <li class="nav-item "> <a class="nav-link" href="/Teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> </header> <article> <p>An up-to-date list is available in my <a href="https://scholar.google.com/citations?user=IgKAJBwAAAAJ" target="_blank" rel="noopener noreferrer">Google Scholar</a>.</p> <div class="publications"> <h5 class="year">2024</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#E00909"><a href="">SR4RS</a></abbr></div> <div id="PereiraSansSR4RS" class="col-sm-8"> <div class="title">A comprehensive overview of satellite image fusion: From classical model-based to cutting-edge deep learning approaches</div> <div class="author"> Pereira-Sánchez, I., Sans, E., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Super Resolution for Remote Sensing,</em> Springer Nature, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Earth observation satellites usually acquire a high-resolution image with a very limited number of spectral bands along with a lower-resolution image that accurately encodes the spectral responses of objects in the scene. Satellite image fusion, also known as pansharpening or hypersharpening depending on the characteristics of the data, aims to combine the spatial and spectral information into a single high-resolution multispectral or hyperspectral image. The resulting image is then used in a wide variety of remote sensing applications, for which low ground sampling distance and detailed description of the chemical-physical composition of the objects may be required. In this chapter, we review the state of the art of satellite image fusion, emphasizing the crucial role that the modeling of the problem plays in performance and analyzing how deep learning has changed the paradigm. This study enables comprehending the evolution of various approaches and their respective outcomes. We establish a fair comparison process, standardizing a general strategy to train and evaluate fusion methods. Quantitative and qualitative comparisons are conducted on several datasets with distinct resolutions and sensor characteristics. The source code used for the comparison are published freely for non-commercial use.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">PereiraSansSR4RS</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pereira-S\'anchez, I. and Sans, E. and Navarro, J. and Duran, J.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A comprehensive overview of satellite image fusion: From classical model-based to cutting-edge deep learning approaches}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Super Resolution for Remote Sensing}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Kawulok, M. and Kawulok, J. and Smolka, B. and Emre Celebi, M.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">MIGARS</a></abbr></div> <div id="PereiraSansMIGARS2024" class="col-sm-8"> <div class="title">A simple nonlocal back-projection unfolded network for pansharpening</div> <div class="author"> Pereira-Sánchez, I., Sans, E., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Machine Intelligence for Geoanalytics and Remote Sensing (MIGARS),</em> Wellington, New Zealand, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Pansharpening is the fusion process that combines the geometry of a high-resolution panchromatic image with the spectral information encoded in a low-resolution multispectral image. We introduce a back-projection method to minimize the reconstruction error between the target image and the output produced by the Brovey pansharpening model. We replace the back-projection kernel with a residual network that incorporates a nonlocal module, exploiting self-similarity and built upon the multi-head attention mechanism. Experimental validation showcases that our method achieves state-of-the-art results.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">PereiraSansMIGARS2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A simple nonlocal back-projection unfolded network for pansharpening}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pereira-S\'anchez, I. and Sans, E. and Navarro, J. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Machine Intelligence for Geoanalytics and Remote Sensing (MIGARS)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Wellington, New Zealand}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">MIGARS</a></abbr></div> <div id="CostaSansMIGARS2024" class="col-sm-8"> <div class="title">Improving marine litter segmentation with limited resolution satellite imagery</div> <div class="author"> Costa, A., Sans, E., Pereira-Sánchez, I.,  <em>Duran, J.</em>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Machine Intelligence for Geoanalytics and Remote Sensing (MIGARS),</em> Wellington, New Zealand, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This work proposes a learning-based semantic segmentation approach to detect floating plastic litter on the marine surface using Sentinel-2 satellite data. We adopt a convolutional network with a reduced number of parameters and, apart from the multispectral data, we feed specific indexes to assist plastic segmentation. Our approach compares favorably against other learning-based methods tailored for the same task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">CostaSansMIGARS2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving marine litter segmentation with limited resolution satellite imagery}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Costa, A. and Sans, E. and Pereira-S\'anchez, I. and Duran, J. and Navarro, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Machine Intelligence for Geoanalytics and Remote Sensing (MIGARS)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Wellington, New Zealand}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">VISAPP</a></abbr></div> <div id="HammondSbertVISAPP2024" class="col-sm-8"> <div class="title">Two nonlocal variational models for Retinex image decomposition</div> <div class="author"> Hammond, F.W., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP),</em> Rome, Italy, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Unlike the human vision system, which is able to recognize color under different illumination conditions, the information captured by a digital camera highly depends on the light reflected by the objects in the scene. The Retinex theory assumes that a digital image can be decomposed into illumination and reflectance component. In this work, we propose two variational models to solve the ill-posed inverse problem of estimating illumination and reflectance from a given observation. In both approaches, nonlocal regularization exploiting image self-similarities is used to estimate the reflectance, since it is assumed to contain fine details and texture. The difference between both models comes from the selected prior for the illumination component. The Sobolev norm, which promots smooth solutions, and the total variation semi-norm, which favours piecewise constant solutions, are independently proposed. A theoretical analysis of the resulting energy functionals is provided in suitable functional spaces.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">HammondSbertVISAPP2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Two nonlocal variational models for Retinex image decomposition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hammond, F.W. and Sbert, C. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Rome, Italy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">VISAPP</a></abbr></div> <div id="PereiraSansVISAPP2024" class="col-sm-8"> <div class="title">Beyond variational models and self-similarity in super-resolution: Unfolding models and multi-head attention</div> <div class="author"> Pereira-Sánchez, I., Sans, E., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP),</em> Rome, Italy, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Classical variational methods for solving image processing problems are more interpretable and flexible than pure deep learning approaches, but their performance is limited by the use of rigid priors. Deep unfolding networks combine the strengths of both by unfolding the steps of the optimization algorithm used to estimate the minimizer of an energy functional into a deep learning framework. In this paper, we propose an unfolding approach to extend a variational model exploiting self-similarity of natural images in the data fidelity term for single-image super-resolution. The proximal, downsampling and upsampling operators are written in terms of a neural network specifically designed for each purpose. Moreover, we include a new multi-head attention module to replace the nonlocal term in the original formulation. A comprehensive evaluation covering a wide range of sampling factors and noise realizations proves the benefits of the proposed unfolding techniques. The model shows to better preserve image geometry while being robust to noise.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">PereiraSansVISAPP2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond variational models and self-similarity in super-resolution: Unfolding models and multi-head attention}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pereira-S\'anchez, I. and Sans, E. and Navarro, J. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Rome, Italy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">VISAPP</a></abbr></div> <div id="TorresSbertVISAPP2024" class="col-sm-8"> <div class="title">Combining total variation and nonlocal variational models for low-light image enhancement</div> <div class="author"> Torres, D., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP),</em> Rome, Italy, 2024 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The rapid expansion of technology has popularized the use of image processing techniques in several fields. However, images captured under low-light conditions impose significant limitations on the performance of these methods. Therefore, improving the quality of these images by discounting the effect of the illumination is crucial. In this paper, we present a low-light image enhancement method based on the Retinex theory. Our approach estimates illumination and reflectance in two steps. First, the illumination is obtained as the minimizer of an energy functional involving total variation regularization, which favours piecewise smooth solutions. Afterwards, the reflectance component is computed as the minimizer of an energy involving contrast-invariant nonlocal regularization and a fidelity term preserving the largest gradients of the input image.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TorresSbertVISAPP2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Combining total variation and nonlocal variational models for low-light image enhancement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Torres, D. and Sbert, C. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Rome, Italy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2023</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">CVPR</a></abbr></div> <div id="MifdalTomasCVPR2023" class="col-sm-8"> <div class="title">Deep unfolding for hypersharpening using a high-frequency injection module</div> <div class="author"> <a href="https://scholar.google.co.uk/citations?hl=en&amp;user=fWqPN8oAAAAJ&amp;view_op=list_works&amp;authuser=1" target="_blank" rel="noopener noreferrer">Mifdal, J.</a>, Tomás-Cruz, M., <a href="https://alessandrosebastianelli.github.io/" target="_blank" rel="noopener noreferrer">Sebastianelli, A.</a>, Coll, B.,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Earthvision Workshop,</em> Vancouver, Canada, 2023 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10208956" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2023-cvpr.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>The fusion of multi-source data with different spatial and spectral resolutions is a crucial task in many remote sensing and computer vision applications. Model-based fusion methods are more interpretable and flexible than pure data-driven learning networks, however, their performance depends greatly on the established fusion model and the hand-crafted prior. In this work, we propose an end-to-end trainable model-based network for hyperspectral and panchromatic image fusion. We introduce an energy functional that takes into account classical observation models and incorporates a high-frequency details injection constraint. The resulting optimization function is solved by a forward-backward splitting algorithm and unfolded into a deep-learning framework that uses two modules trained in parallel to ensure both data observation fitting and constraint compliance. Extensive experiments are conducted on the remote-sensing hyperspectral PRISMA dataset and on the CAVE dataset, proving the superiority of the proposed deep unfolding network both qualitatively and quantitatively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">MifdalTomasCVPR2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep unfolding for hypersharpening using a high-frequency injection module}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mifdal, J. and Tom{\'a}s-Cruz, M. and Sebastianelli, A. and Coll, B. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Earthvision Workshop}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2105--2114}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPRW59228.2023.00204}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">IGARSS</a></abbr></div> <div id="TomasMifdalIGARSS2023" class="col-sm-8"> <div class="title">End-to-end shallow network for variational pansharpening</div> <div class="author"> Tomás-Cruz, M., <a href="https://scholar.google.co.uk/citations?hl=en&amp;user=fWqPN8oAAAAJ&amp;view_op=list_works&amp;authuser=1" target="_blank" rel="noopener noreferrer">Mifdal, J.</a>, Coll, B.,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS),</em> Pasadena, USA, 2023 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10282759" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2023-igarss.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Pansharpening aims to fuse the geometry of a high-resolution panchromatic image with the color information of a low-resolution multispectral image to generate a high-resolution multispectral image. Classical variational methods are more interpretable and flexible than pure deep learning approaches, but their performance is limited by the use of rigid priors. In this paper, we efficiently combine both techniques by introducing a shallow residual network to learn the regularization term of a variational pansharpening model. The proposed energy includes the classical observation model for the multispectral data and a constraint to preserve the geometry encoded in the panchromatic. The experiments demonstrate that our method achieves state-of-the-art results.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TomasMifdalIGARSS2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{End-to-end shallow network for variational pansharpening}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tom{\'a}s-Cruz, M. and Mifdal, J. and Coll, B. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Pasadena, USA}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6803-6806}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IGARSS52108.2023.10282759}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2022</h5> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">ICIP</a></abbr></div> <div id="PereiraNavarroDuranICIP2022" class="col-sm-8"> <div class="title">What if image self-similarity can be better exploited in data fidelity terms?</div> <div class="author"> Pereira-Sánchez, I., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the IEEE International Conference on Image Processing (ICIP),</em> Bordeaux, France, 2022 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9897904" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2022-whatif.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>In this work, we introduce a novel variational model for image restoration. In particular, we study the suitability of exploit- ing self-similarity of natural images in the fidelity term. Tra- ditionally, this cue has been used for the regularization term, promoting to align similarities in the degraded image with similarities in the restored one. In contrast, our proposed non- local data-fidelity term penalizes deviations of patches after having suffered from the degradation process if they are sim- ilar in the degraded image. Experiments on super-resolution, denoising and depth filtering show the competitiveness of this new formulation with respect to traditional nonlocal regular- ization terms and recent learning-based methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">PereiraNavarroDuranICIP2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{What if image self-similarity can be better exploited in data fidelity terms?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pereira-S\'anchez, I. and Navarro, J. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Image Processing (ICIP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Bordeaux, France}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h5 class="year">2021</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">Mathematics</a></abbr></div> <div id="MifdalCollFromentDuranMDPI2021" class="col-sm-8"> <div class="title">Variational fusion of hyperspectral data by non-local filtering</div> <div class="author"> <a href="https://scholar.google.co.uk/citations?hl=en&amp;user=fWqPN8oAAAAJ&amp;view_op=list_works&amp;authuser=1" target="_blank" rel="noopener noreferrer">Mifdal, J.</a>, Coll, B., Froment, J.,  and <em>Duran, J.</em> </div> <div class="periodical"> <em>Mathematics,</em> vol. 9, pp. 1265, 2021 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/2227-7390/9/11/1265" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.mdpi.com/2227-7390/9/11/1265/pdf?version=1624439568" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The fusion of multisensor data has attracted a lot of attention in computer vision, particularly among the remote sensing community. Hyperspectral image fusion consists in merging the spectral information of a hyperspectral image with the geometry of a multispectral one in order to infer an image with high spatial and spectral resolutions. In this paper, we propose a variational fusion model with a nonlocal regularization term that encodes patch-based filtering conditioned to the geometry of the multispectral data. We further incorporate a radiometric constraint that injects the high frequencies of the scene into the fused product with a band per band modulation according to the energy levels of the multispectral and hyperspectral images. The proposed approach proved robust to noise and aliasing. The experimental results demonstrate the performance of our method with respect to the state-of-the-art techniques on data acquired by commercial hyperspectral cameras and Earth observation satellites.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">MifdalCollFromentDuranMDPI2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Variational fusion of hyperspectral data by non-local filtering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mifdal, J. and Coll, B. and Froment, J. and Duran, J.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1265}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/math9111265}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">SIIMS</a></abbr></div> <div id="DuranNavarroBuadesSIIMS2021" class="col-sm-8"> <div class="title">Variational densification and refinement of registration maps</div> <div class="author"> <em>Duran, J.</em>, <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a> </div> <div class="periodical"> <em>SIAM Journal on Imaging Sciences,</em> vol. 14, pp. 879–912, 2021 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://epubs.siam.org/doi/abs/10.1137/20M1379113" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Local patch-based algorithms for image registration fail to accurately match points in areas not discriminative enough, mainly textureless regions. These methods normally involve a validation process and provide a non-completely dense solution. In this paper, we propose a novel refinement and completion approach for registration. The proposed model combines single image nonlocal densification with classical variational image registration. We associate a total variation regularization with a nonlocal term to provide a smooth solution leveraging the image geometry. We show experiments on public stereo and optical flow datasets to filter and densify incomplete depth maps and motion fields. Extensive comparisons against existing and state-of-the-art depth/motion fields densification approaches demonstrate the competitive performance of the introduced method. Additionally, we illustrate how our method can deal with other tasks, such as filtering and interpolation of depth maps from RGBD data and depth upsampling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DuranNavarroBuadesSIIMS2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Variational densification and refinement of registration maps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Navarro, J. and Buades, A.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Imaging Sciences}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{879--912}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1137/20M1379113}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2019</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">TCSVT</a></abbr></div> <div id="BuadesDuranTCSVT2019" class="col-sm-8"> <div class="title">CFA video denoising and demosaicking chain via spatio-temporal patch-based filtering</div> <div class="author"> <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology,</em> vol. 30, pp. 4143–4157, 2019 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/8917679" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="http://arxiv.org/abs/1812.11207" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Demosaicking and denoising are key steps in the camera imaging chain for both images and videos. The reconstruction errors during these stages will have undesirable effects on the final result if not handled properly. Demosaicking provokes the spatial and color correlation of noise, which is afterwards enhanced by the processing pipeline. This structured noise generally degrades the image quality and, for dark scenes with low signal to noise ratio, prevents the correct interpretation of the image. When trying to mitigate such structured noise on already processed data, denoising methods attenuate details and texture. We present a video processing chain, consisting of a novel strategy for the removal of noise at the camera sensor and a novel video demosaicking algorithm. In both cases, a spatio-temporal patch-based filter with motion compensation is introduced. The experimental results, including real examples, illustrate the performance of the proposed chain, avoiding the creation of interpolation artefacts and colored spots.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">BuadesDuranTCSVT2019</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Buades, A. and Duran, J.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Circuits and Systems for Video Technology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4143--4157}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TCSVT.2019.2956691}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">IJCV</a></abbr></div> <div id="BuadesDuranNavarroIJCV2019" class="col-sm-8"> <div class="title">Motion-compensated spatio-temporal filtering for multi-image and multimodal super-resolution</div> <div class="author"> <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a>,  <em>Duran, J.</em>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a> </div> <div class="periodical"> <em>International Journal of Computer Vision,</em> vol. 127, pp. 1474–1500, 2019 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s11263-019-01200-5" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>The classical multi-image super-resolution model assumes that the super-resolved image is related to the low-resolution frames by warping, convolution and downsampling. State-of-the-art algorithms either use explicit registration to fuse the information for each pixel in its trajectory or exploit spatial and temporal similarities. We propose to combine both ideas, making use of inter-frame motion and exploiting spatio-temporal redundancy with patch-based techniques. We introduce a non-linear filtering approach that combines patches from several frames not necessarily belonging to the same pixel trajectory. The selection of candidate patches depends on a motion-compensated 3D distance, which is robust to noise and aliasing. The selected 3D volumes are then sliced per frame, providing a collection of 2D patches which are finally averaged depending on their similarity to the reference one. This makes the upsampling strategy robust to flow inaccuracies and occlusions. Total variation and nonlocal regularization are used in the deconvolution stage. The experimental results demonstrate the state-of-the-art performance of the proposed method for the super-resolution of videos and light-field images. We also adapt our approach to multimodal sequences when some additional data at the desired resolution is available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">BuadesDuranNavarroIJCV2019</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Motion-compensated spatio-temporal filtering for multi-image and multimodal super-resolution}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Buades, A. and Duran, J. and Navarro, J.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Computer Vision}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{127}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1474--1500}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s11263-019-01200-5}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2018</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">LGRS</a></abbr></div> <div id="DuranBuadesLGRS2018" class="col-sm-8"> <div class="title">Restoration of pansharpened images by conditional filtering in the PCA domain</div> <div class="author"> <em>Duran, J.</em>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a> </div> <div class="periodical"> <em>IEEE Geoscience and Remote Sensing Letters,</em> vol. 16, pp. 442–446, 2018 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/8527530" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="http://arxiv.org/abs/1710.00672" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Pansharpening techniques aim at fusing a low-spatial resolution multispectral (MS) image with a higher spatial resolution panchromatic (PAN) image to produce an MS image at high spatial resolution. Despite significant progress in the field, spectral and spatial distortions might still compromise the quality of the results. We introduce a restoration strategy to mitigate artifacts of fused products. After applying the principal component analysis transform to a pansharpened image, the chromatic components are filtered conditionally to the geometry of PAN. The structural component is then replaced by the locally histogram-matched PAN for spatial enhancement. Experimental results illustrate the efficiency of the proposed restoration chain.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DuranBuadesLGRS2018</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Buades, A.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Restoration of pansharpened images by conditional filtering in the PCA domain}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Geoscience and Remote Sensing Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{442--446}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LGRS.2018.2873654}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">ICIP</a></abbr></div> <div id="NavarroDuranBuadesICIP2018" class="col-sm-8"> <div class="title">Filtering and interpolation of inaccurate and incomplete depth maps</div> <div class="author"> <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=xnEqXFwAAAAJ" target="_blank" rel="noopener noreferrer">Navarro, J.</a>,  <em>Duran, J.</em>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a> </div> <div class="periodical"> In <em> Proceedings of the IEEE International Conference on Image Processing (ICIP),</em> Athens, Greece, 2018 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/8451854" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2018-flowfilt.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Local algorithms for stereo fail to match accurately points in areas not discriminative enough, mainly texture-less regions. We propose a method for filtering incorrect depth estimates and filling in those areas which have not been matched. The proposed model combines total variation regularization with a non local term taking advantage of image self similarity. The method can be easily generalized to deal with other tasks, as for example stereo upsampling. The method is compared with state-of-the-art stereo methods, showing competitive results on the Middlebury stereo database.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NavarroDuranBuadesICIP2018</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Filtering and interpolation of inaccurate and incomplete depth maps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Navarro, J. and Duran, J. and Buades, A.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Image Processing (ICIP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Athens, Greece}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1533--1537}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP.2018.8451854}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">ICIP</a></abbr></div> <div id="BuadesDuranICIP2018" class="col-sm-8"> <div class="title">Joint denoising and demosaicking of raw video sequences</div> <div class="author"> <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a>,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the IEEE International Conference on Image Processing (ICIP),</em> Athens, Greece, 2018 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/8451853" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2018-raw.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>The demosaicking provokes the spatial and color correlation of noise, which is afterwards enhanced by the imaging pipeline. The correct removal previous or simultaneously with the demosaicking process is not usually considered in the literature. We present a novel joint demosaicking and denoising algorithm for image sequences. The proposed algorithm uses a spatio-temporal patch method modifying all pixels, including those of the Bayer CFA. However, only original values are considered for averaging. The experimentation, including real examples, illustrates how a joint denoising and demosaicking algorithm avoids the creation of artifacts and colored spots in the final image.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">BuadesDuranICIP2018</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Buades, A. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Image Processing (ICIP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Athens, Greece}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2172--2176}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP.2018.8451853}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">ICIP</a></abbr></div> <div id="MifdalCollDuranICIP2018" class="col-sm-8"> <div class="title">A variational formulation for hyperspectral and multispectral image fusion</div> <div class="author"> <a href="https://scholar.google.co.uk/citations?hl=en&amp;user=fWqPN8oAAAAJ&amp;view_op=list_works&amp;authuser=1" target="_blank" rel="noopener noreferrer">Mifdal, J.</a>, Coll, B.,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the IEEE International Conference on Image Processing (ICIP),</em> Athens, Greece, 2018 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/8451609" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2018-hyperspectral.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>We propose a new convex variational model for hyperspectral and multispectral image fusion. Our approach introduces nonlocal regularization conditioned to the geometry of the multispectral image and incorporates a constraint forcing the fusion product and the multispectral data to share modulated high frequencies. The proposed method is compared with state-of-the-art fusion techniques, showing competitive results for several quality metrics on different data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">MifdalCollDuranICIP2018</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A variational formulation for hyperspectral and multispectral image fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mifdal, J. and Coll, B. and Duran, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Image Processing (ICIP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Athens, Greece}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3328--3332}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP.2018.8451609}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2017</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">ISPRS</a></abbr></div> <div id="DuranBuadesCollSbertBlanchetISPRS2017" class="col-sm-8"> <div class="title">A survey of pansharpening methods with a new band-decoupled variational model</div> <div class="author"> <em>Duran, J.</em>, <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a>, Coll, B., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a>,  and Blanchet, G. </div> <div class="periodical"> <em>ISPRS Journal of Photogrammetry and Remote Sensing,</em> vol. 125, pp. 78–105, 2017 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dx.doi.org/10.1016/j.isprsjprs.2016.12.013" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S0924271616306657/pdfft?casa_token=iHojHN8ocBgAAAAA:R0vEbRU1LouGvBxTtoSuXCpmGGqrRCwI-zYiOXwgmiqKnH7b9bYoThm_OOhWJxXWlEEAOyZS9hg&amp;md5=d57aeca44d8b00941d37e766ff169098&amp;pid=1-s2.0-S0924271616306657-main.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="http://arxiv.org/abs/1606.05703" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Most satellites decouple the acquisition of a panchromatic image at high spatial resolution from the acquisition of a multispectral image at lower spatial resolution. Pansharpening is a fusion technique used to increase the spatial resolution of the multispectral data while simultaneously preserving its spectral information. In this paper, we consider pansharpening as an optimization problem minimizing a cost function with a nonlocal regularization term. The energy functional which is to be minimized decouples for each band, thus permitting the application to misregistered spectral components. This requirement is achieved by dropping the, commonly used, assumption that relates the spectral and panchromatic modalities by a linear transformation. Instead, a new constraint that preserves the radiometric ratio between the panchromatic and each spectral component is introduced. An exhaustive performance comparison of the proposed fusion method with several classical and state-of-the-art pansharpening techniques illustrates its superiority in preserving spatial details, reducing color distortions, and avoiding the creation of aliasing artifacts.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DuranBuadesCollSbertBlanchetISPRS2017</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Buades, A. and Coll, B. and Sbert, C. and Blanchet, G.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A survey of pansharpening methods with a new band-decoupled variational model}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ISPRS Journal of Photogrammetry and Remote Sensing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{125}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{78--105}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.isprsjprs.2016.12.013}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">BMVC</a></abbr></div> <div id="BuadesDuranBMVC2017" class="col-sm-8"> <div class="title">Flow based video super-resolution with spatio-temporal patch similarity</div> <div class="author"> A., Buades,  and <em>Duran, J.</em> </div> <div class="periodical"> In <em> Proceedings of the British Machine Vision Conference (BMVC),</em> London, UK, 2017 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.bmva.org/bmvc/2017/papers/paper147/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="http://www.bmva.org/bmvc/2017/papers/paper147/paper147.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/assets/pdf/posters/2017-bmvc.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>The goal of super-resolution is to fuse several low-resolution images of the same scene into a single one with increased resolution. The classical formulation assumes that the super-resolved image is related to the low-resolution frames by warping, convolution and subsampling. Algorithms divide into those using explicit registration and those avoiding it. The ﬁrst ones combine for each pixel the information in its estimated trajectory. The second ones exploit both spatial and temporal redundancy. We propose to combine both ideas, making use of optical ﬂow and exploiting spatio-temporal redundancy with patch-based techniques. The proposed non-linear ﬁltering takes into account patch similarities, automatically correcting the ﬂow inaccuracies and avoiding the need of occlusion detection. Total variation and nonlocal regularization are used for the deconvolution stage.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">BuadesDuranBMVC2017</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{A., Buades and Duran, J.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flow based video super-resolution with spatio-temporal patch similarity}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the British Machine Vision Conference (BMVC)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{London, UK}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{656.1--656.12}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5244/C.31.147}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">VISAPP</a></abbr></div> <div id="DuranBuadesVISAPP2017" class="col-sm-8"> <div class="title">Nonlocal regularizing constraints in variational optical flow</div> <div class="author"> <em>Duran, J.</em>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP),</em> Porto, Portugal, 2017 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0006098501510161" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://pdfs.semanticscholar.org/a315/2daa0e0a15c05bc6d495cf1f5848928bf72c.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/assets/pdf/slides/2017-visapp.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Optical flow methods try to estimate a dense correspondence field describing the motion of the objects in an image sequence. We introduce novel nonlocal regularizing constraints for variational optical flow computation. While the use of similarity weights has been restricted to the regularization term so far, the proposed data terms permit to implicitly use the image geometry in order to regularize the flow and better locate motion discontinuities. The experimental results illustrate the superiority of the new constraints with respect to the classical brightness constancy assumption as well as to nonlocal regularization strategies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DuranBuadesVISAPP2017</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Nonlocal regularizing constraints in variational optical flow}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Buades, A.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Computer Vision, Theory and Applications (VISAPP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Porto, Portugal}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{151--161}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5220/0006098501510161}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2016</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">ICIP</a></abbr></div> <div id="DuranBuadesCollSbertBlanchetICIP2016" class="col-sm-8"> <div class="title">Pansharpening by a nonlocal channel-decoupled variational method</div> <div class="author"> <em>Duran, J.</em>, <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a>, Coll, B., <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a>,  and Blanchet, G. </div> <div class="periodical"> In <em> Proceedings of the IEEE International Conference on Image Processing (ICIP),</em> Phoenix, USA, 2016 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/7533179" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2016-pansharpening.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Common satellite imagery products consist of a panchromatic image at high spatial resolution and several misregistered spectral bands at lower resolution. Pansharpening is the fusion process by which a high-resolution multispectral image is inferred. We propose a variational model for which pan-sharpening is defined as an optimization problem minimizing a cost function with nonlocal regularization. We incorporate a new term preserving the radiometric ratio between the panchromatic and each spectral band. The resulting model is channel-decoupled, thus permitting the application to misregistered spectral data. The experimental results illustrate the superiority of the proposed method to preserve spatial details, reduce color artifacts, and avoid aliasing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DuranBuadesCollSbertBlanchetICIP2016</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Buades, A. and Coll, B. and Sbert, C. and Blanchet, G.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Pansharpening by a nonlocal channel-decoupled variational method}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Image Processing (ICIP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Phoenix, USA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4339--4343}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP.2016.7533179}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">SIIMS</a></abbr></div> <div id="DuranMoellerSbertCremersSIIMS2016" class="col-sm-8"> <div class="title">Collaborative total variation: A general framework for vectorial TV models</div> <div class="author"> <em>Duran, J.</em>, <a href="https://sites.google.com/site/michaelmoellermath/" target="_blank" rel="noopener noreferrer">Moeller, M.</a>, <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a>,  and <a href="https://vision.in.tum.de/members/cremers" target="_blank" rel="noopener noreferrer">Cremers, D.</a> </div> <div class="periodical"> <em>SIAM Journal on Imaging Sciences,</em> vol. 9, pp. 116–151, 2016 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://epubs.siam.org/doi/10.1137/15M102873X" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="http://arxiv.org/abs/1508.01308" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/posters/2016-CTV.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="http://www.ipol.im/pub/art/2016/141/DMSC_TVdenoising.tgz" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://demo.ipol.im/demo/141/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Even after two decades, the total variation (TV) remains one of the most popular regularizations for image processing problems and has sparked a tremendous amount of research, particularly on moving from scalar to vector-valued functions. In this paper, we consider the gradient of a color image as a three-dimensional matrix or tensor with dimensions corresponding to the spatial extent, the intensity differences between neighboring pixels, and the spectral channels. The smoothness of this tensor is then measured by taking different norms along the different dimensions. Depending on the types of these norms, one obtains very different properties of the regularization, leading to novel models for color images. We call this class of regularizations collaborative total variation (CTV). On the theoretical side, we characterize the dual norm, the subdifferential, and the proximal mapping of the proposed regularizers. We further prove, with the help of the generalized concept of singular vectors, that an l^∞ channel coupling makes the most prior assumptions and has the greatest potential to reduce color artifacts. Our practical contributions consist of an extensive experimental section, where we compare the performance of a large number of collaborative TV methods for inverse problems such as denoising, deblurring, and inpainting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DuranMoellerSbertCremersSIIMS2016</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Moeller, M. and Sbert, C. and Cremers, D.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Collaborative total variation: A general framework for vectorial TV models}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Imaging Sciences}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{116--151}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1137/15M102873X}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2015</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">EMMCVPR</a></abbr></div> <div id="DuranMoellerSbertCremersNLTV2015" class="col-sm-8"> <div class="title">A novel framework for nonlocal vectorial total variation based on l^p,q,r norms</div> <div class="author"> <em>Duran, J.</em>, <a href="https://sites.google.com/site/michaelmoellermath/" target="_blank" rel="noopener noreferrer">Moeller, M.</a>, <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a>,  and <a href="https://vision.in.tum.de/members/cremers" target="_blank" rel="noopener noreferrer">Cremers, D.</a> </div> <div class="periodical"> In <em> Proceedings of the International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition (EMMCVPR),</em> Hong Kong, 2015 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-14612-6_11" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/slides/2015-emmcvpr.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>In this paper, we propose a novel framework for restoring color images using nonlocal total variation (NLTV) regularization. We observe that the discrete local and nonlocal gradient of a color image can be viewed as a 3D matrix or tensor with dimensions corresponding to the spatial extend, the differences to other pixels, and the color channels. Based on this observation we obtain a new class of NLTV methods by penalizing the l^p,q,r norm of this 3D tensor. Interestingly, this unifies several local color total variation (TV) methods in a single framework. We show in several numerical experiments on image denoising and deblurring that a stronger coupling of different color channels – particularly, a coupling with the l^∞ norm – yields superior reconstruction results.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DuranMoellerSbertCremersNLTV2015</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Moeller, M. and Sbert, C. and Cremers, D.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A novel framework for nonlocal vectorial total variation based on $l^{p,q,r}$ norms}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition (EMMCVPR)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Hong Kong}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Lecture Notes in Computer Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8932}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{141--154}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-319-14612-6\_11}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">IPI</a></abbr></div> <div id="CollDuranSbertIPI2015" class="col-sm-8"> <div class="title">Half-linear regularization for nonconvex image restoration models</div> <div class="author"> Coll, B.,  <em>Duran, J.</em>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a> </div> <div class="periodical"> <em>Inverse Problems and Imaging,</em> vol. 9, pp. 337–370, 2015 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.aimsciences.org/article/doi/10.3934/ipi.2015.9.337" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Image restoration is the problem of recovering an original image from an observation of it in order to extract the most meaningful information. In this paper, we study this problem from a variational point of view through the minimization of energies composed of a quadratic data-fidelity term and a nonsmooth nonconvex regularization term. In the discrete setting, existence of minimizer is proved for arbitrary linear operators. For this kind of problems, fully segmented solutions can be found by minimizing objective nonconvex functionals. We propose a dual formulation of the model by introducing an auxiliary variable with a double function. On one hand, it marks the edges and it ensures their preservation from smoothing. On the other hand, it makes the criterion half-linear in the sense that the dual energy depends linearly on the gradient of the image to be recovered. This leads to design an efficient optimization algorithm with wide applicability to several image restoration tasks such as denoising and deconvolution. Finally, we present experimental results and we compare them with TV-based image restoration algorithms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">CollDuranSbertIPI2015</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Coll, B. and Duran, J. and Sbert, C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Half-linear regularization for nonconvex image restoration models}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Inverse Problems and Imaging}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{337--370}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3934/ipi.2015.9.337}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2014</h5> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#3EA055"><a href="">ICIP</a></abbr></div> <div id="CollDuranSbertICIP2014" class="col-sm-8"> <div class="title">An algorithm for nonconvex functional minimization and applications to image restoration</div> <div class="author"> Coll, B.,  <em>Duran, J.</em>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a> </div> <div class="periodical"> In <em> Proceedings of the IEEE International Conference on Image Processing (ICIP),</em> Paris, France, 2014 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/7025922" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/posters/2014-halflinear.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>In this paper, we propose a new dual algorithm for the minimization of discrete nonconvex functionals, called half-linear regularization. Our approach alternates the calculation of a explicit weight with the minimization of a convex functional with respect to the solution. This minimization corresponds to the weighted total variation which is solved via the well-known Chambolle’s algorithm. Finally, we present experimental results by applying it to some image restoration problems as denoising and deconvolution.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">CollDuranSbertICIP2014</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Coll, B. and Duran, J. and Sbert, C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An algorithm for nonconvex functional minimization and applications to image restoration}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Image Processing (ICIP)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Paris, France}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4547--4551}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP.2014.7025922}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">TIP</a></abbr></div> <div id="DuranBuadesTIP2014" class="col-sm-8"> <div class="title">Self-similarity and spectral correlation adaptive algorithm for color demosaicking</div> <div class="author"> <em>Duran, J.</em>,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a> </div> <div class="periodical"> <em>IEEE Transactions on Image Processing,</em> vol. 23, pp. 4031–4040, 2014 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/6862910/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="http://www.ipol.im/pub/art/2015/145/DB_demosaicking_code.tar.gz" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://demo.ipol.im/demo/145/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Most common cameras use a CCD sensor device measuring a single color per pixel. The other two color values of each pixel must be interpolated from the neighboring pixels in the so-called demosaicking process. State-of-the-art demosaicking algorithms take advantage of interchannel correlation locally selecting the best interpolation direction. These methods give impressive results except when local geometry cannot be inferred from neighboring pixels or channel correlation is low. In these cases, they create interpolation artifacts. We introduce a new algorithm involving nonlocal image self-similarity in order to reduce interpolation artifacts when local geometry is ambiguous. The proposed algorithm introduces a clear and intuitive manner of balancing how much channel-correlation must be taken advantage of. Comparison shows that the proposed algorithm gives state-of-the-art methods in several image bases.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DuranBuadesTIP2014</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Buades, A.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Image Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4031--4040}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TIP.2014.2341928}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">SIIMS</a></abbr></div> <div id="DuranBuadesCollSbertSIIMS2014" class="col-sm-8"> <div class="title">A nonlocal variational model for pansharpening image fusion</div> <div class="author"> <em>Duran, J.</em>, <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=on1RaQQAAAAJ" target="_blank" rel="noopener noreferrer">Buades, A.</a>, Coll, B.,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a> </div> <div class="periodical"> <em>SIAM Journal on Imaging Sciences,</em> vol. 7, pp. 761–796, 2014 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://epubs.siam.org/doi/abs/10.1137/130928625" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="http://www.ipol.im/pub/art/2014/98/Pansharpening.zip" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://demo.ipol.im/demo/98/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Pansharpening refers to the fusion process of inferring a high-resolution multispectral image from a high-resolution panchromatic image and a low-resolution multispectral one. In this paper we propose a new variational method for pansharpening which incorporates a nonlocal regularization term and two fidelity terms, one describing the relation between the panchromatic image and the high-resolution spectral channels and the other one preserving the colors from the low-resolution modality. The nonlocal term is based on the image self-similarity principle applied to the panchromatic image. The existence and uniqueness of minimizer for the described functional is proved in a suitable space of weighted integrable functions. Although quite successful in terms of relative error, state-of-the-art pansharpening methods introduce relevant color artifacts. These spectral distortions can be significantly reduced by involving the image self-similarity. Extensive comparisons with state-of-the-art algorithms are performed.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DuranBuadesCollSbertSIIMS2014</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Buades, A. and Coll, B. and Sbert, C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A nonlocal variational model for pansharpening image fusion}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Imaging Sciences}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{761--796}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1137/130928625}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h5 class="year">2013</h5> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:nil"><a href="">IPOL</a></abbr></div> <div id="DuranCollSbertIPOL2013" class="col-sm-8"> <div class="title">Chambolle’s projection algorithm for total variation denoising</div> <div class="author"> <em>Duran, J.</em>, Coll, B.,  and <a href="https://scholar.google.co.uk/citations?hl=en&amp;authuser=1&amp;user=9k9tTxsAAAAJ" target="_blank" rel="noopener noreferrer">Sbert, C.</a> </div> <div class="periodical"> <em>Image Processing On Line,</em> vol. 3, pp. 301–321, 2013 </div> <p style="font-size:0.8rem; color: rgb(199, 139, 9)"> </p> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.ipol.im/pub/art/2013/61/?utm_source=doi" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="http://www.ipol.im/pub/art/2013/61/article.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://www.ipol.im/pub/art/2013/61/CDS_ChambolleTV.tgz" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="http://demo.ipol.im/demo/61/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Denoising is the problem of removing the inherent noise from an image. The standard noise model is additive white Gaussian noise, where the observed image f is related to the underlying true image u by the degradation model f=u+η, and η is supposed to be at each pixel independently and identically distributed as a zero-mean Gaussian random variable. Since this is an ill-posed problem, Rudin, Osher and Fatemi introduced the total variation as a regularizing term. It has proved to be quite efficient for regularizing images without smoothing the boundaries of the objects. This paper focuses on the simple description of the theory and on the implementation of Chambolle’s projection algorithm for minimizing the total variation of a grayscale image. Furthermore, we adapt the algorithm to the vectorial total variation for color images. The implementation is described in detail and its parameters are analyzed and varied to come up with a reliable implementation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DuranCollSbertIPOL2013</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Duran, J. and Coll, B. and Sbert, C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Chambolle's projection algorithm for total variation denoising}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Image Processing On Line}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{301--321}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5201/ipol.2013.61}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Joan Duran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>